\documentclass[english, a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\usepackage{url}
\usepackage[english]{babel}
\usepackage[autostyle=true,english=american]{csquotes}
\MakeOuterQuote{"}
\usepackage[style=ieee, sorting=none]{biblatex}
\usepackage[hidelinks]{hyperref}

\addbibresource{references.bib}
\addbibresource{zotero.bib}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}

\title{
    Research Plan for CSE3000 Research Project\\
        {\large \textit{Parametrized Similarity Metrics for CNF problem instances generated by fuzzing for \#SAT solvers}}
}
\author{Cristian Soare}

\begin{document}
\maketitle

\section*{Background of the research}

The project's research centers on testing and debugging tools for the Boolean Satisfiability Problem (SAT), an NP-hard problem that involves determining whether a satisfying assignment exists for a given Boolean formula \parencite{Biere2009}. Specifically, it will be focusing on applying fuzzing and delta-debugging \parencite{zellerYesterdayMyProgram1999} techniques to Model Counting (\#SAT). Model Counting differs from SAT in that it finds out the number of unique assignments, rather than just determining if a satisfying assignment exists. There are also different variants of model counting: weighted, projected and weighted projected (WMC/PMC/WPMC) \parencite{Biere2009}. Regardless of how these model counters are implemented, they are usually used for validating the correctness of hardware and software systems \parencite{Duenas2017,Latour2022,Baluta2019}. Therefore, it is reasonable to expect a high level of correctness and efficiency from these tools. However, that might not always be the case. This is why testing and debugging tools are essential for the developers building these solvers, ensuring reliability and robustness.

\section*{Research Question}

The specific research question is:\\

\textit{"How can we develop and validate a parameterized similarity metric for quantifying structural differences between distributions of CNF problem instances generated by fuzzers, to ensure comprehensive testing coverage for (weighted) model counting solvers?"}\\

This question aims to address the challenge of evaluating the effectiveness of fuzzing techniques for model counting solvers. By developing a similarity metric, the structural differences between CNF instances generated by fuzzers can be quantified, providing a way to measure the effectiveness of the testing process. This metric will be parameterized to allow for customization based on the specific requirements of the model counting solver being tested.\\

The research question's focus on developing a parameterized similarity metric naturally defines the key variables. The independent variables represent the inputs that can controlled and manipulate to develop the metric. One of these inputs relates to the structural properties of CNF problem instances for SAT or \#SAT solvers. Standard SAT problem characteristics identified by \textcite{xuSATzillaPortfoliobasedAlgorithm2008} include variable-clause ratio, balance of horn clauses, structural parameters like treewidth and modularity, as well as graph-based metrics derived from variable interaction graphs. These need to be analyzed alongside more recent structural measurements like the entropy-based metrics proposed by \textcite{zhangStructuralEntropyMeasurement2021} and community structure parameters introduced by \textcite{ganianNewWidthParameters2021}.

Another crucial input concerns the fuzzing techniques to be investigated. Early in the research, we need to study and evaluate various fuzzing approaches for their applicability to CNF generation. The fundamental principles and strategies of fuzzing are outlined in \textcite{Zeller2024, Zeller2023} and need to be investigated. Modern techniques also include rare-path guided fuzzing \parencite{sahaRarePathGuided2023}, quantitative fuzzing methods \parencite{nollerQFuzzQuantitativeFuzzing2021}, and coverage-guided approaches \parencite{bohmeEstimatingResidualRisk2021}.

The final input comprises the parameters that must be defined for the Parametrized Similarity Metric. These parameters will need to be either empirically evaluated or "learned" to enable differentiation between various types of model counters. Future research into model counter characteristics and behavior will inform the parameter design \parencite{dudekDPMCWeightedModel2020, dudekADDMCWeightedModel2020, dudekParallelWeightedModel2021, fichteProofsPropositionalModel2022}.

The dependent variable - the similarity metric's output - requires validation against established code coverage concepts \parencite{Zeller2024, Zeller2023}. The core hypothesis is that CNF instances deemed similar by the metric should produce comparable coverage patterns when testing model counters, while dissimilar instances should explore different solver behaviors. However, defining "good" coverage metrics for \#SAT solvers presents its own challenges \parencite{usmanTestMCTestingModel2020, carvalhoSpecBCFuzzFuzzingLTL2024} and requires further investigation during the research.

Confounding variables could also impact the validity of the results if not properly controlled. These include implementation differences between fuzzers, effects of random seeds, issues with instance scaling, and choices in feature selection. Addressing these factors in the analysis is crucial for the research project's accuracy.

\section*{Research Sub-questions}

\textbf{Structural Analysis:}
\begin{itemize}
    \item What structural properties of CNF formulas (e.g., variable-clause ratio, horn clause balance, treewidth) are most relevant for characterizing model counting problems?
    \item How can novel CNF parameters be incorporated into similarity measurements?
    \item What mathematical properties should an ideal similarity metric possess to capture both local and global CNF characteristics?
\end{itemize}

\textbf{Fuzzing Strategy:}
\begin{itemize}
    \item What defines a "comprehensive" set of test cases specifically for weighted model counting?
    \item How can traditional fuzzing techniques be used to ensure diversity in generated instances?
    \item How can novel fuzzing approaches be adapted for CNF generation?
\end{itemize}

\textbf{Metric Development:}
\begin{itemize}
    \item How can we mathematically represent and compare distributions of CNF problem instances?
    \item What parameters are necessary to differentiate between various types of model counters?
    \item How can we ensure the metric scales appropriately with problem size while maintaining meaningful comparisons?
\end{itemize}

\textbf{Validation \& Integration:}
\begin{itemize}
    \item How can we validate that our similarity metric correlates with bug detection patterns?
    \item Where in the SharpVelvet fuzzing pipeline should similarity metrics be integrated?
    \item What empirical evidence would demonstrate the effectiveness of parameterized metrics versus fixed metrics?
\end{itemize}

\textbf{Practical Application:}
\begin{itemize}
    \item How can solver developers use these metrics to improve testing coverage?
    \item What is the computational overhead of applying these metrics in practice?
    \item How can the metrics be tuned to specific model counting variants (WMC/PMC/WPMC)?
\end{itemize}

\section*{Method}

This research project should follow a mixed-methods approach, combining deductive reasoning with experimental validation. The first phase involves developing a theoretical framework through comprehensive analysis of existing literature on CNF structural properties, fuzzing techniques, and model counting behaviors (as discussed in the previous section).

Then, the implementation (validation) phase will utilize the SharpVelvet\footnote{https://github.com/meelgroup/SharpVelvet} repository. We propose developing a Python-based analysis tool for extracting structural features from CNF instances. This analyzer will be integrated with a configurable similarity metric system, allowing for parameterized comparison of SAT formula characteristics. To validate the effectiveness of these similarity metrics, I propose implementing a controlled "buggy" \#SAT solver with introduced defects. This approach will enable systematic evaluation of the framework's bug detection capabilities through known fault patterns.

The experimental validation consists of generating diverse CNF instances using multiple fuzzers, applying the developed similarity metrics, and testing these instances against the controlled solver. Success will be measured through statistical correlation between similarity scores and bug detection patterns, as well as coverage analysis of the generated test cases. The implementation will be compared against existing \#SAT solvers \parencite{dudekDPMCWeightedModel2020,dudekADDMCWeightedModel2020} to ensure practical relevance.

A crucial component of the methodology involves optimizing the weights of the parameterized similarity metric. This will be approached through two parallel strategies: an empirical trial-and-error process to establish baseline parameters, followed by possibly an automated optimization using machine learning techniques such as Gaussian Process-based hyperparameter optimization \parencite{shahriariTakingHumanOut2016}. This dual approach will allow us to compare hand-tuned parameters against mathematically optimized ones, potentially revealing insights about which CNF features or model counter properties are most relevant for similarity assessment.

One of the primary challenges related to this research relies on establishing meaningful correlations between similarity metrics and bug detection patterns. Given the time constraints of the project, I will address this by starting with simple metrics and incrementally adding complexity. Computational resource limitations will be managed through efficient implementations and strategic sampling approaches.

\section*{Planning of the research project}

Throughout the project, weekly supervisor meetings will be held to discuss progress and receive feedback. The schedule below also includes mandatory course components: ACS training sessions (Weeks 3-6), Responsible Research lectures (Weeks 3-4), and coaching sessions (Weeks 7-9).

\begin{tabular}{|p{0.12\textwidth}|p{0.78\textwidth}|}
\hline
\textbf{Week} & \textbf{Activities and Deliverables} \\
\hline
Week 1 & 
$\bullet$ Complete Info Literacy II Course and Research Methods bootcamp \\
& $\bullet$ Literature review: CNF properties, fuzzing techniques, model counting \\
& $\bullet$ Research plan and presentation preparation \\
& $\bullet$ Weekly supervisor meeting \\
& \textbf{Deliverables:} Research plan, presentation slides, Info Literacy Assignment \\
\hline
Week 2 & 
$\bullet$ Develop theoretical framework for similarity metrics \\
& $\bullet$ Analysis of existing CNF structural properties \\
& $\bullet$ Begin Introduction and Related Work sections \\
& $\bullet$ Weekly supervisor meeting \\
& \textbf{Deliverables:} ACS Assignment 1 \\
\hline
Week 3 & 
$\bullet$ Complete CNF structural properties analysis \\
& $\bullet$ Design initial similarity metric framework \\
& $\bullet$ Setup development environment for SharpVelvet \\
& $\bullet$ Continue writing Related Work section \\
& $\bullet$ Weekly supervisor meeting \\
& $\bullet$ Attend ACS training and Responsible Research lecture \\
\hline
Week 4 & 
$\bullet$ Implement basic CNF analyzer \\
& $\bullet$ Design test cases for validation \\
& $\bullet$ Begin "buggy" \#SAT solver implementation \\
& $\bullet$ Write Methodology section \\
& $\bullet$ Weekly supervisor meeting \\
& $\bullet$ Attend ACS training and Responsible Research lecture \\
& \textbf{Deliverables:} ACS Assignments 2a and 2b \\
\hline
Week 5 & 
$\bullet$ Implement core similarity metrics \\
& $\bullet$ Initial validation testing \\
& $\bullet$ Write Implementation section \\
& $\bullet$ Weekly supervisor meeting \\
& $\bullet$ Attend ACS training \\
& $\bullet$ Prepare midterm presentation \\
& \textbf{Deliverables:} Midterm presentation, ACS Assignment 3 \\
\hline
Week 6 & 
$\bullet$ Complete initial parameter tuning experiments \\
& $\bullet$ Document preliminary results \\
& $\bullet$ Complete first draft (Introduction, Related Work, Method, Initial Results) \\
& $\bullet$ Weekly supervisor meeting \\
& $\bullet$ Attend ACS training \\
& \textbf{Deliverables:} Paper Draft v1 for peer review \\
\hline
Week 7-8 & 
$\bullet$ Process peer feedback \\
& $\bullet$ Refine implementation based on feedback \\
& $\bullet$ Complete parameter optimization \\
& $\bullet$ Finalize Results and Discussion sections \\
& $\bullet$ Weekly supervisor meetings \\
& $\bullet$ Attend coaching sessions \\
& \textbf{Deliverables:} Paper Draft v2 for professor review \\
\hline
Week 9 & 
$\bullet$ Incorporate professor feedback \\
& $\bullet$ Final implementation refinements \\
& $\bullet$ Complete paper revisions \\
& $\bullet$ Weekly supervisor meeting \\
& $\bullet$ Attend coaching session \\
& \textbf{Deliverables:} Final paper \\
\hline
Week 10 & 
$\bullet$ Prepare final presentation and poster \\
& $\bullet$ Create implementation demonstration \\
& $\bullet$ Weekly supervisor meeting \\
& \textbf{Deliverables:} Research poster, Final presentation, Project demo \\
\hline
\end{tabular}

\pagebreak

\printbibliography

\end{document}
