@book{a.biereHandbookSatisfiabilitySecond2021,
  title = {Handbook of {{Satisfiability}} : {{Second Edition}}},
  shorttitle = {Handbook of {{Satisfiability}}},
  author = {{A. Biere} and {M. Heule} and {H. van Maaren}},
  year = {2021},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications Ser}}},
  number = {v.336},
  publisher = {IOS Press},
  address = {[N.p.]},
  urldate = {2024-11-13},
  abstract = {Propositional logic has been recognized throughout the centuries as one of the cornerstones of reasoning in philosophy and mathematics. Over time, its formalization into Boolean algebra was accompanied by the recognition that a wide range of combinatorial problems can be expressed as propositional satisfiability (SAT) problems. Because of this dual role, SAT developed into a mature, multi-faceted scientific discipline, and from the earliest days of computing a search was underway to discover how to solve SAT problems in an automated fashion. This book, the Handbook of Satisfiability, is the second, updated and revised edition of the book first published in 2009 under the same name. The handbook aims to capture the full breadth and depth of SAT and to bring together significant progress and advances in automated solving. Topics covered span practical and theoretical research on SAT and its applications and include search algorithms, heuristics, analysis of algorithms, hard instances, randomized formulae, problem encodings, industrial applications, solvers, simplifiers, tools, case studies and empirical results. SAT is interpreted in a broad sense, so as well as propositional satisfiability, there are chapters covering the domain of quantified Boolean formulae (QBF), constraints programming techniques (CSP) for word-level problems and their propositional encoding, and satisfiability modulo theories (SMT). An extensive bibliography completes each chapter. This second edition of the handbook will be of interest to researchers, graduate students, final-year undergraduates, and practitioners using or contributing to SAT, and will provide both an inspiration and a rich resource for their work. Edmund Clarke, 2007 ACM Turing Award Recipient:'SAT solving is a key technology for 21st century computer science.'Donald Knuth, 1974 ACM Turing Award Recipient:'SAT is evidently a killer app, because it is key to the solution of so many other problems.'Stephen Cook, 1982 ACM Turing Award Recipient:'The SAT problem is at the core of arguably the most fundamental question in computer science: What makes a problem hard?'},
  isbn = {978-1-64368-160-3},
  langid = {english},
  keywords = {Algebra Boolean--Congresses,Computer algorithms--Congresses,COMPUTERS / Computer Science,Decision making--Congresses,Propositional calculus--Congresses},
  file = {/home/csoare/Zotero/storage/PH9ICPGP/A. Biere et al. - 2021 - Handbook of Satisfiability  Second Edition.pdf}
}

@misc{balutaQuantitativeVerificationNeural2019,
  title = {Quantitative {{Verification}} of {{Neural Networks And}} Its {{Security Applications}}},
  author = {Baluta, Teodora and Shen, Shiqi and Shinde, Shweta and Meel, Kuldeep S. and Saxena, Prateek},
  year = {2019},
  month = jun,
  number = {arXiv:1906.10395},
  eprint = {1906.10395},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.10395},
  urldate = {2024-11-14},
  abstract = {Neural networks are increasingly employed in safety-critical domains. This has prompted interest in verifying or certifying logically encoded properties of neural networks. Prior work has largely focused on checking existential properties, wherein the goal is to check whether there exists any input that violates a given property of interest. However, neural network training is a stochastic process, and many questions arising in their analysis require probabilistic and quantitative reasoning, i.e., estimating how many inputs satisfy a given property. To this end, our paper proposes a novel and principled framework to quantitative verification of logical properties specified over neural networks. Our framework is the first to provide PAC-style soundness guarantees, in that its quantitative estimates are within a controllable and bounded error from the true count. We instantiate our algorithmic framework by building a prototype tool called NPAQ that enables checking rich properties over binarized neural networks. We show how emerging security analyses can utilize our framework in 3 concrete point applications: quantifying robustness to adversarial inputs, efficacy of trojan attacks, and fairness/bias of given neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  file = {/home/csoare/Zotero/storage/BKUNSMR9/Baluta et al. - 2019 - Quantitative Verification of Neural Networks And its Security Applications.pdf;/home/csoare/Zotero/storage/VVMLTFMN/1906.html}
}

@book{bessierePrinciplesPracticeConstraint2007,
  title = {Principles and {{Practice}} of {{Constraint Programming}} -- {{CP}} 2007: 13th {{International Conference}}, {{CP}} 2007, {{Providence}}, {{RI}}, {{USA}}, {{September}} 23-27, 2007. {{Proceedings}}},
  shorttitle = {Principles and {{Practice}} of {{Constraint Programming}} -- {{CP}} 2007},
  editor = {Bessi{\`e}re, Christian and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  year = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {4741},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-74970-7},
  urldate = {2024-11-14},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-540-74969-1 978-3-540-74970-7},
  langid = {english},
  keywords = {algorithms,assistive technology,autonom,complexity,constraint logic programming,constraint networks,constraint programming,constraint propagation,constraint satisfaction,constraint solving,E-Learning,heuristics,Planning,Variable,visualization},
  file = {/home/csoare/Zotero/storage/TCPHCVFA/Bessière - 2007 - Principles and Practice of Constraint Programming – CP 2007 13th International Conference, CP 2007,.pdf}
}

@book{biereHandbookSatisfiability2021,
  title = {Handbook of Satisfiability},
  author = {Biere, Armin and Heule, Marijn and van Maaren, Hans},
  year = {2021},
  series = {Frontiers in Artificial Intelligence and Applications},
  edition = {Second edition},
  number = {v. 336},
  publisher = {IOS Press},
  address = {Amsterdam},
  urldate = {2024-11-13},
  isbn = {978-1-64368-161-0},
  langid = {english},
  keywords = {Handbook,Model Couners Basics,SAT}
}

@misc{birnbaumGoodOldDavisPutnam2011,
  title = {The {{Good Old Davis-Putnam Procedure Helps Counting Models}}},
  author = {Birnbaum, E. and Lozinskii, E. L.},
  year = {2011},
  month = jun,
  number = {arXiv:1106.0218},
  eprint = {1106.0218},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1106.0218},
  urldate = {2024-11-15},
  abstract = {As was shown recently, many important AI problems require counting the number of models of propositional formulas. The problem of counting models of such formulas is, according to present knowledge, computationally intractable in a worst case. Based on the Davis-Putnam procedure, we present an algorithm, CDP, that computes the exact number of models of a propositional CNF or DNF formula F. Let m and n be the number of clauses and variables of F, respectively, and let p denote the probability that a literal l of F occurs in a clause C of F, then the average running time of CDP is shown to be O(nm{\textasciicircum}d), where d=-1/log(1-p). The practical performance of CDP has been estimated in a series of experiments on a wide variety of CNF formulas.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/csoare/Zotero/storage/AACMPMWH/Birnbaum and Lozinskii - 2011 - The Good Old Davis-Putnam Procedure Helps Counting Models.pdf;/home/csoare/Zotero/storage/Q3WNAUXJ/1106.html}
}

@inproceedings{bohmeEstimatingResidualRisk2021,
  title = {Estimating Residual Risk in Greybox Fuzzing},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {B{\"o}hme, Marcel and Liyanage, Danushka and W{\"u}stholz, Valentin},
  year = {2021},
  month = aug,
  series = {{{ESEC}}/{{FSE}} 2021},
  pages = {230--241},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3468264.3468570},
  urldate = {2024-11-15},
  abstract = {For any errorless fuzzing campaign, no matter how long, there is always some residual risk that a software error would be discovered if only the campaign was run for just a bit longer. Recently, greybox fuzzing tools have found widespread adoption. Yet, practitioners can only guess when the residual risk of a greybox fuzzing campaign falls below a specific, maximum allowable threshold. In this paper, we explain why residual risk cannot be directly estimated for greybox campaigns, argue that the discovery probability (i.e., the probability that the next generated input increases code coverage) provides an excellent upper bound, and explore sound statistical methods to estimate the discovery probability in an ongoing greybox campaign. We find that estimators for blackbox fuzzing systematically and substantially under-estimate the true risk. An engineer---who stops the campaign when the estimators purport a risk below the maximum allowable risk---is vastly misled. She might need execute a campaign that is orders of magnitude longer to achieve the allowable risk. Hence, the key challenge we address in this paper is adaptive bias: The probability to discover a specific error actually increases over time. We provide the first probabilistic analysis of adaptive bias, and introduce two novel classes of estimators that tackle adaptive bias. With our estimators, the engineer can decide with confidence when to abort the campaign.},
  isbn = {978-1-4503-8562-6},
  file = {/home/csoare/Zotero/storage/SSP5XJPN/Böhme et al. - 2021 - Estimating residual risk in greybox fuzzing.pdf}
}

@inproceedings{carvalhoSpecBCFuzzFuzzingLTL2024,
  title = {{{SpecBCFuzz}}: {{Fuzzing LTL Solvers}} with {{Boundary Conditions}}},
  shorttitle = {{{SpecBCFuzz}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 46th {{International Conference}} on {{Software Engineering}}},
  author = {Carvalho, Luiz and Degiovanni, Renzo and Cordy, Maxime and Aguirre, Nazareno and Le Traon, Yves and Papadakis, Mike},
  year = {2024},
  month = apr,
  series = {{{ICSE}} '24},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3597503.3639087},
  urldate = {2024-11-15},
  abstract = {LTL solvers check the satisfiability of Linear-time Temporal Logic (LTL) formulas and are widely used for verifying and testing critical software systems. Thus, potential bugs in the solvers' implementations can have a significant impact. We present SpecBCFuzz, a fuzzing method for finding bugs in LTL solvers, that is guided by boundary conditions (BCs), corner cases whose (un)satisfiability depends on rare traces. SpecBCFuzz implements a search-based algorithm that fuzzes LTL formulas giving relevance to BCs. It integrates syntactic and semantic similarity metrics to explore the vicinity of the seeded formulas with BCs. We evaluate SpecBCFuzz on 21 different configurations (including the latest and past releases) of four mature and state-of-the-art LTL solvers (NuSMV, Black, Aalta, and PLTL) that implement a diverse set of satisfiability algorithms. SpecBCFuzz produces 368,716 bug-triggering formulas, detecting bugs in 18 out of the 21 solvers' configurations we study. Overall, SpecBCFuzz reveals: soundness issues (wrong answers given by a solver) in Aalta and PLTL; crashes, e.g., segmentation faults, in NuSMV, Black and Aalta; flaky behaviors (different responses across re-runs of the solver on the same formula) in NuSMV and Aalta; performance bugs (large time performance degradation between successive versions of the solver on the same formula) in Black, Aalta and PLTL; and no bug in NuSMV BDD (all versions), suggesting that the latter is currently the most robust solver.},
  isbn = {9798400702174},
  file = {/home/csoare/Zotero/storage/7BQZ4JLI/Carvalho et al. - 2024 - SpecBCFuzz Fuzzing LTL Solvers with Boundary Conditions.pdf}
}

@article{chakrabortyDistributionAwareSamplingWeighted2014,
  title = {Distribution-{{Aware Sampling}} and {{Weighted Model Counting}} for {{SAT}}},
  author = {Chakraborty, Supratik and Fremont, Daniel and Meel, Kuldeep and Seshia, Sanjit and Vardi, Moshe},
  year = {2014},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {28},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v28i1.8990},
  urldate = {2024-11-15},
  abstract = {Given a CNF formula and a weight for each assignment of values tovariables, two natural problems are weighted model counting anddistribution-aware sampling of satisfying assignments.  Both problems have a wide variety of important applications.  Due to the inherentcomplexity of the exact versions of the problems, interest has focusedon solving them approximately.  Prior work in this area scaled only tosmall problems in practice, or failed to provide strong theoreticalguarantees, or employed a computationally-expensive most-probable-explanation (\{{\textbackslash}MPE\}) queries that assumes prior knowledge of afactored representation of the weight distribution. We identify a novel parameter,{\textbackslash}emph\{tilt\}, which is the ratio of the maximum weight of satisfying assignment to minimum weightof satisfying assignment and present anovel approach that works with a black-box oracle for weights ofassignments and requires only an \{{\textbackslash}NP\}-oracle (in practice, a \{{\textbackslash}SAT\}-solver) to solve both thecounting and sampling problems when the tilt is small.  Our approach  provides strong theoretical guarantees, and scales toproblems involving several thousand variables. We also show that theassumption of small tilt can be significantly relaxed while improving computational efficiency if a factored representation of the weights is known.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {machine learning},
  file = {/home/csoare/Zotero/storage/BE37YLYH/Chakraborty et al. - 2014 - Distribution-Aware Sampling and Weighted Model Counting for SAT.pdf}
}

@article{chaviraProbabilisticInferenceWeighted2008,
  title = {On Probabilistic Inference by Weighted Model Counting},
  author = {Chavira, Mark and Darwiche, Adnan},
  year = {2008},
  month = apr,
  journal = {Artificial Intelligence},
  volume = {172},
  number = {6},
  pages = {772--799},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2007.11.002},
  urldate = {2024-11-15},
  abstract = {A recent and effective approach to probabilistic inference calls for reducing the problem to one of weighted model counting (WMC) on a propositional knowledge base. Specifically, the approach calls for encoding the probabilistic model, typically a Bayesian network, as a propositional knowledge base in conjunctive normal form (CNF) with weights associated to each model according to the network parameters. Given this CNF, computing the probability of some evidence becomes a matter of summing the weights of all CNF models consistent with the evidence. A number of variations on this approach have appeared in the literature recently, that vary across three orthogonal dimensions. The first dimension concerns the specific encoding used to convert a Bayesian network into a CNF. The second dimensions relates to whether weighted model counting is performed using a search algorithm on the CNF, or by compiling the CNF into a structure that renders WMC a polytime operation in the size of the compiled structure. The third dimension deals with the specific properties of network parameters (local structure) which are captured in the CNF encoding. In this paper, we discuss recent work in this area across the above three dimensions, and demonstrate empirically its practical importance in significantly expanding the reach of exact probabilistic inference. We restrict our discussion to exact inference and model counting, even though other proposals have been extended for approximate inference and approximate model counting.},
  keywords = {Bayesian networks,Compilation,Exact inference,Weighted model counting},
  file = {/home/csoare/Zotero/storage/SL9UJ9YD/Chavira and Darwiche - 2008 - On probabilistic inference by weighted model counting.pdf;/home/csoare/Zotero/storage/E48N6G6H/S0004370207001889.html}
}

@inproceedings{dallaAutomatedSATProblem2021,
  title = {Automated {{SAT Problem Feature Extraction}} Using {{Convolutional Autoencoders}}},
  booktitle = {2021 {{IEEE}} 33rd {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {Dalla, Marco and Visentin, Andrea and O'Sullivan, Barry},
  year = {2021},
  month = nov,
  pages = {232--239},
  issn = {2375-0197},
  doi = {10.1109/ICTAI52525.2021.00039},
  urldate = {2024-11-14},
  abstract = {The Boolean Satisfiability Problem (SAT) was the first known NP-complete problem and has a very broad literature focusing on it. It has been applied successfully to various real-world problems, such as scheduling, planning and cryptography. SAT problem feature extraction plays an essential role in this field. SAT solvers are complex, fine-tuned systems that exploit problem structure. The ability to represent/encode a large SAT problem using a compact set of features has broad practical use in instance classification, algorithm portfolios, and solver configuration. The performance of these techniques relies on the ability of feature extraction to convey helpful information. Researchers often craft these features "by hand" to capture particular structures of the problem. Instead, in this paper, we extract features using semi-supervised deep learning. We train a convolutional autoencoder (AE) to compress the SAT problem into a limited latent space and reconstruct it minimizing the reconstruction error. The latent space projection should preserve much of the structural features of the problem. We compare our approach to a set of features commonly used for algorithm selection. Firstly, we train classifiers on the projection to predict if the problems are satisfiable or not. If the compression conveys valuable information, a classifier should be able to take correct decisions. In the second experiment, we check if the classifiers can identify the original problem that was encoded as SAT. The empirical analysis shows that the autoencoder is able to represent problem features in a limited latent space efficiently, as well as convey more information than current feature extraction methods.},
  keywords = {boolean satisfiability,Classification algorithms,CNF encoding,convolutional autoencoders,deep learning,Deep learning,Encoding,feature extraction,Feature extraction,Neural networks,Prediction algorithms,Predictive models,satisfiability prediction},
  file = {/home/csoare/Zotero/storage/YYS6KZCM/Dalla et al. - 2021 - Automated SAT Problem Feature Extraction using Convolutional Autoencoders.pdf}
}

@inproceedings{dilkasGeneratingRandomInstances2023,
  title = {Generating {{Random Instances}} of~{{Weighted Model Counting}}},
  booktitle = {Integration of {{Constraint Programming}}, {{Artificial Intelligence}}, and {{Operations Research}}},
  author = {Dilkas, Paulius},
  editor = {Cire, Andre A.},
  year = {2023},
  pages = {395--416},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-33271-5_26},
  abstract = {Weighted model counting (WMC) is an extension of propositional model counting with applications to probabilistic inference and other areas of artificial intelligence. In recent experiments, WMC algorithms perform similarly overall but with significant differences on specific subsets of benchmarks. A good understanding of the differences in the performance of algorithms requires identifying key characteristics that favour some algorithms over others. In this paper, we introduce a random model for WMC instances with a parameter that influences primal treewidth---the parameter most commonly used to characterise the difficulty of an instance. We then use this model to experimentally compare the performance of WMC algorithms c2d, Cachet, d4, DPMC, and miniC2D. Using these random instances, we show that the easy-hard-easy pattern is different for algorithms based on dynamic programming and algebraic decision diagrams than for all other solvers. We also show how all WMC algorithms scale exponentially with respect to primal treewidth and how this scalability varies across algorithms and densities. Finally, we combine insights from experiments involving both random and competition instances to determine how the best-performing WMC algorithm varies depending on clause density and primal treewidth.},
  isbn = {978-3-031-33271-5},
  langid = {english},
  keywords = {Parameterised complexity,Random model,Weighted model counting},
  file = {/home/csoare/Zotero/storage/RV7577RI/Dilkas - 2023 - Generating Random Instances of Weighted Model Counting.pdf}
}

@article{dudekADDMCWeightedModel2020,
  title = {{{ADDMC}}: {{Weighted Model Counting}} with {{Algebraic Decision Diagrams}}},
  shorttitle = {{{ADDMC}}},
  author = {Dudek, Jeffrey and Phan, Vu and Vardi, Moshe},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {02},
  pages = {1468--1476},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i02.5505},
  urldate = {2024-11-15},
  abstract = {We present an algorithm to compute exact literal-weighted model counts of Boolean formulas in Conjunctive Normal Form. Our algorithm employs dynamic programming and uses Algebraic Decision Diagrams as the main data structure. We implement this technique in ADDMC, a new model counter. We empirically evaluate various heuristics that can be used with ADDMC. We then compare ADDMC to four state-of-the-art weighted model counters (Cachet, c2d, d4, and miniC2D) on 1914 standard model counting benchmarks and show that ADDMC significantly improves the virtual best solver.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/home/csoare/Zotero/storage/RE76EGCB/Dudek et al. - 2020 - ADDMC Weighted Model Counting with Algebraic Decision Diagrams.pdf}
}

@inproceedings{dudekDPMCWeightedModel2020,
  title = {{{DPMC}}: {{Weighted Model Counting}} by {{Dynamic Programming}} on {{Project-Join Trees}}},
  shorttitle = {{{DPMC}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}}},
  author = {Dudek, Jeffrey M. and Phan, Vu H. N. and Vardi, Moshe Y.},
  editor = {Simonis, Helmut},
  year = {2020},
  pages = {211--230},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-58475-7_13},
  abstract = {We propose a unifying dynamic-programming framework to compute exact literal-weighted model counts of formulas in conjunctive normal form. At the center of our framework are project-join trees, which specify efficient project-join orders to apply additive projections (variable eliminations) and joins (clause multiplications). In this framework, model counting is performed in two phases. First, the planning phase constructs a project-join tree from a formula. Second, the execution phase computes the model count of the formula, employing dynamic programming as guided by the project-join tree. We empirically evaluate various methods for the planning phase and compare constraint-satisfaction heuristics with tree-decomposition tools. We also investigate the performance of different data structures for the execution phase and compare algebraic decision diagrams with tensors. We show that our dynamic-programming model-counting framework DPMC is competitive with the state-of-the-art exact weighted model counters Cachet, c2d, d4, and miniC2D.},
  isbn = {978-3-030-58475-7},
  langid = {english},
  keywords = {Early projection,Factored representation,Treewidth},
  file = {/home/csoare/Zotero/storage/5AYLSVC9/Dudek et al. - 2020 - DPMC Weighted Model Counting by Dynamic Programming on Project-Join Trees.pdf}
}

@misc{dudekParallelWeightedModel2021,
  title = {Parallel {{Weighted Model Counting}} with {{Tensor Networks}}},
  author = {Dudek, Jeffrey M. and Vardi, Moshe Y.},
  year = {2021},
  month = jun,
  number = {arXiv:2006.15512},
  eprint = {2006.15512},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.15512},
  urldate = {2024-11-15},
  abstract = {A promising new algebraic approach to weighted model counting makes use of tensor networks, following a reduction from weighted model counting to tensor-network contraction. Prior work has focused on analyzing the single-core performance of this approach, and demonstrated that it is an effective addition to the current portfolio of weighted-model-counting algorithms. In this work, we explore the impact of multi-core and GPU use on tensor-network contraction for weighted model counting. To leverage multiple cores, we implement a parallel portfolio of tree-decomposition solvers to find an order to contract tensors. To leverage a GPU, we use TensorFlow to perform the contractions. We compare the resulting weighted model counter on 1914 standard weighted model counting benchmarks and show that it significantly improves the virtual best solver.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/home/csoare/Zotero/storage/ICLH4WHY/Dudek and Vardi - 2021 - Parallel Weighted Model Counting with Tensor Networks.pdf;/home/csoare/Zotero/storage/JFDQ4F6X/2006.html}
}

@article{duenas-osorioCountingBasedReliabilityEstimation2017,
  title = {Counting-{{Based Reliability Estimation}} for {{Power-Transmission Grids}}},
  author = {{Duenas-Osorio}, Leonardo and Meel, Kuldeep and Paredes, Roger and Vardi, Moshe},
  year = {2017},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {31},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v31i1.11178},
  urldate = {2024-11-14},
  abstract = {Modern society is increasingly reliant on the functionality of infrastructure facilities and utility services. Consequently, there has been surge of interest in the problem of quantification of system reliability, which is known to be \#P-complete. Reliability also contributes to the resilience of systems, so as to effectively make them   bounce back after contingencies. Despite diverse progress, most techniques to estimate system reliability and resilience remain computationally expensive. In this paper, we investigate how recent advances in hashing-based approaches to counting can be exploited to improve computational techniques for system reliability.The primary contribution of this paper is a novel framework, RelNet, that reduces the problem of computing reliability for a given network to counting the number of satisfying assignments of a {$\Sigma$}11 formula, which is amenable to recent hashing-based techniques developed for counting satisfying assignments of SAT formula. We then apply RelNet to ten real world power-transmission grids across different cities in the U.S. and are able to obtain, to the best of our knowledge, the first theoretically sound a priori estimates of reliability between several pairs of nodes of interest.  Such estimates will help managing uncertainty and support rational decision making for community resilience.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {approxmc},
  file = {/home/csoare/Zotero/storage/UUCIY3CQ/Duenas-Osorio et al. - 2017 - Counting-Based Reliability Estimation for Power-Transmission Grids.pdf}
}

@misc{EfficientAlgorithmUnit,
  title = {An {{Efficient Algorithm}} for {{Unit Propagation}}},
  journal = {ResearchGate},
  urldate = {2024-11-16},
  abstract = {PDF {\textbar} this paper) show that our algorithm works better than any other previous algorithms. The basic idea of our new algorithm is to check a linear list... {\textbar} Find, read and cite all the research you need on ResearchGate},
  howpublished = {https://www.researchgate.net/publication/2508830\_An\_Efficient\_Algorithm\_for\_Unit\_Propagation},
  langid = {english},
  file = {/home/csoare/Zotero/storage/36R8WSL8/An Efficient Algorithm for Unit Propagation.pdf;/home/csoare/Zotero/storage/TF8AKWMF/2508830_An_Efficient_Algorithm_for_Unit_Propagation.html}
}

@inproceedings{fichteProofsPropositionalModel2022,
  title = {Proofs for {{Propositional Model Counting}}},
  booktitle = {25th {{International Conference}} on {{Theory}} and {{Applications}} of {{Satisfiability Testing}} ({{SAT}} 2022)},
  author = {Fichte, Johannes K. and Hecher, Markus and Roland, Valentin},
  year = {2022},
  pages = {30:1-30:24},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  doi = {10.4230/LIPIcs.SAT.2022.30},
  urldate = {2024-11-15},
  abstract = {Although propositional model counting (\#SAT) was long considered too hard to be practical, today's highly efficient solvers facilitate applications in probabilistic reasoning, reliability estimation, quantitative design space exploration, and more. The current trend of solvers growing more capable every year is likely to continue as a diverse range of algorithms are explored in the field. However, to establish model counters as reliable tools like SAT-solvers, correctness is as critical as speed. As in the nature of complex systems, bugs emerge as soon as the tools are widely used. To identify and avoid bugs, explain decisions, and provide trustworthy results, we need verifiable results. We propose a novel system for certifying model counts. We show how proof traces can be generated for exact model counters based on dynamic programming, counting CDCL with component caching, and knowledge compilation to Decision-DNNF, which are the predominant techniques in today's exact implementations. We provide proof-of-concepts for emitting proofs and a parallel trace checker. Based on this, we show the feasibility of using certified model counting in an empirical experiment.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/home/csoare/Zotero/storage/RHF2QNDR/Fichte et al. - 2022 - Proofs for Propositional Model Counting.pdf}
}

@inproceedings{ganianCommunityStructureInspired2015,
  title = {Community {{Structure Inspired Algorithms}} for {{SAT}} and \#{{SAT}}},
  booktitle = {Theory and {{Applications}} of {{Satisfiability Testing}} -- {{SAT}} 2015},
  author = {Ganian, Robert and Szeider, Stefan},
  editor = {Heule, Marijn and Weaver, Sean},
  year = {2015},
  pages = {223--237},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24318-4_17},
  abstract = {We introduce h-modularity, a structural parameter of CNF formulas, and present algorithms that render the decision problem SAT and the model counting problem \#SAT fixed-parameter tractable when parameterized by h-modularity. The new parameter is defined in terms of a partition of clauses of the given CNF formula into strongly interconnected communities which are sparsely interconnected with each other. Each community forms a hitting formula, whereas the interconnections between communities form a graph of small treewidth. Our algorithms first identify the community structure and then use them for an efficient solution of SAT and \#SAT, respectively. We further show that h-modularity is incomparable with known parameters under which SAT or \#SAT is fixed-parameter tractable.},
  isbn = {978-3-319-24318-4},
  langid = {english},
  keywords = {Community Structure,Constraint Satisfaction Problem,Dual Graph,Equivalence Class,Tree Decomposition},
  file = {/home/csoare/Zotero/storage/CTN6PG8G/Ganian and Szeider - 2015 - Community Structure Inspired Algorithms for SAT and #SAT.pdf}
}

@article{ganianNewWidthParameters2021,
  title = {New Width Parameters for {{SAT}} and \#{{SAT}}},
  author = {Ganian, Robert and Szeider, Stefan},
  year = {2021},
  month = jun,
  journal = {Artificial Intelligence},
  volume = {295},
  pages = {103460},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103460},
  urldate = {2024-11-16},
  abstract = {We study the parameterized complexity of the propositional satisfiability (SAT) and the more general model counting (\#SAT) problems and obtain novel fixed-parameter algorithms that exploit the structural properties of input formulas. In the first part of the paper, we parameterize by the treewidth of the following two graphs associated with CNF formulas: the consensus graph and the conflict graph. Both graphs have as vertices the clauses of the formula; in the consensus graph two clauses are adjacent if they do not contain a complementary pair of literals, while in the conflict graph two clauses are adjacent if they do contain a complementary pair of literals. We show that \#SAT is fixed-parameter tractable when parameterized by the treewidth of the former graph, but SAT is W[1]-hard when parameterized by the treewidth of the latter graph. In the second part of the paper, we turn our attention to a novel structural parameter we call h-modularity which is loosely inspired by the well-established notion of community structure. The new parameter is defined in terms of a partition of clauses of the given CNF formula into strongly interconnected communities which are sparsely interconnected with each other. Each community forms a hitting formula, whereas the interconnections between communities form a graph of small treewidth. Our algorithms first identify the community structure and then use them for an efficient solution of SAT and \#SAT, respectively.},
  keywords = {Community structure,Model counting,Parameterized complexity,SAT,Treewidth},
  file = {/home/csoare/Zotero/storage/WIHUWH35/Ganian and Szeider - 2021 - New width parameters for SAT and #SAT.pdf;/home/csoare/Zotero/storage/4Z7AJYSP/S0004370221000114.html}
}

@inproceedings{kheireddineTuningSATSolvers2022,
  title = {Tuning {{SAT}} Solvers for {{LTL Model Checking}}},
  booktitle = {2022 29th {{Asia-Pacific Software Engineering Conference}} ({{APSEC}})},
  author = {Kheireddine, Anissa and Renault, Etienne and Baarir, Souheib},
  year = {2022},
  month = dec,
  pages = {259--268},
  issn = {2640-0715},
  doi = {10.1109/APSEC57359.2022.00038},
  urldate = {2024-11-17},
  abstract = {Bounded model checking (BMC) aims at checking whether a model satisfies a property. Most of the existing SAT-based BMC approaches rely on generic strategies, which are supposed to work for any SAT problem. The key idea defended in this paper is to tune SAT solvers algorithm using: (1) a static classification based on the variables used to encode the BMC into a Boolean formula; (2) and use the hierarchy of Manna\&Pnueli [33] that classmes any property expressed through Linear-time Temporal Logic (LTL). By combining these two information with the classical Literal Block Distance (LBD) measure [46], we designed a new heuristic, well suited for solving BMC problems. In particular, our work identifies and exploits a new set of relevant (learnt) clauses. We experiment with these ideas by developing a tool dedicated for SAT-based LTL BMC solvers, called BSaLTic. Our experiments over a large database of BMC problems, show promising results. In particular, BSaLTic provides good performance on UNSAT problems. This work highlights the importance of considering the structure of the underlying problem in SAT procedures.},
  keywords = {Adaptation models,Atmospheric measurements,Bounded model-checking,Buildings,Classification algorithms,Databases,Linear Temporal Logic,Model checking,Optimization,Particle measurements,SAT,Structural information},
  file = {/home/csoare/Zotero/storage/PB3KCDJA/Kheireddine et al. - 2022 - Tuning SAT solvers for LTL Model Checking.pdf;/home/csoare/Zotero/storage/TF7PWSTW/10043345.html}
}

@article{latourExactStochasticConstraint2022,
  title = {Exact Stochastic Constraint Optimisation with Applications in Network Analysis},
  author = {Latour, Anna L. D. and Babaki, Behrouz and Fokkinga, Dani{\"e}l and Anastacio, Marie and Hoos, Holger H. and Nijssen, Siegfried},
  year = {2022},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {304},
  pages = {103650},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103650},
  urldate = {2024-11-14},
  abstract = {We present an extensive study of methods for exactly solving stochastic constraint (optimisation) problems (SCPs) in network analysis. These problems are prevalent in science, governance and industry. The first method we study is generic and decomposes stochastic constraints into a multitude of smaller local constraints that are solved using a constraint programming (CP) or mixed-integer programming (MIP) solver. However, many SCPs are formulated on probability distributions with a monotonic property, meaning that adding a positive decision to a partial solution to the problem cannot cause a decrease in solution quality. The second method is specifically designed for solving global stochastic constraints on monotonic probability distributions (SCMDs) in CP. Both methods use knowledge compilation to obtain a decision diagram encoding of the relevant probability distributions, where we focus on ordered binary decision diagrams (OBDDs). We discuss theoretical advantages and disadvantages of these methods and evaluate them experimentally. We observed that global approaches to solving SCMDs outperform decomposition approaches from CP, and perform complementarily to MIP-based decomposition approaches, while scaling much more favourably with instance size. Both methods have many alternative design choices, as both knowledge compilation and constraint solvers are used in a single pipeline. To identify which configurations work best, we apply programming by optimisation. Specifically, we show how an automated algorithm configurator can be used to find optimised configurations of our pipeline. After configuration, our global SCMD solving pipeline outperforms its closest competitor (a MIP-based decomposition pipeline) on all test sets we considered by up to two orders of magnitude in terms of PAR10 scores.},
  keywords = {Automated algorithm configuration,Constraint programming,Global constraints,Monotonic probability distributions,Ordered binary decision diagrams,Probabilistic inference,Probabilistic networks,Stochastic constraints},
  file = {/home/csoare/Zotero/storage/LDNHQVRJ/Latour et al. - 2022 - Exact stochastic constraint optimisation with applications in network analysis.pdf}
}

@inproceedings{leeStatisticalReachabilityAnalysis2023,
  title = {Statistical {{Reachability Analysis}}},
  booktitle = {Proceedings of the 31st {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Lee, Seongmin and B{\"o}hme, Marcel},
  year = {2023},
  month = nov,
  series = {{{ESEC}}/{{FSE}} 2023},
  pages = {326--337},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3611643.3616268},
  urldate = {2024-11-15},
  abstract = {Given a target program state (or statement) s, what is the probability that an input reaches s? This is the quantitative reachability analysis problem. For instance, quantitative reachability analysis can be used to approximate the reliability of a program (where s is a bad state). Traditionally, quantitative reachability analysis is solved as a model counting problem for a formal constraint that represents the (approximate) reachability of s along paths in the program, i.e., probabilistic reachability analysis. However, in preliminary experiments, we failed to run state-of-the-art probabilistic reachability analysis on reasonably large programs. In this paper, we explore statistical methods to estimate reachability probability. An advantage of statistical reasoning is that the size and composition of the program are insubstantial as long as the program can be executed. We are particularly interested in the error compared to the state-of-the-art probabilistic reachability analysis. We realize that existing estimators do not exploit the inherent structure of the program and develop structure-aware estimators to further reduce the estimation error given the same number of samples. Our empirical evaluation on previous and new benchmark programs shows that (i) our statistical reachability analysis outperforms state-of-the-art probabilistic reachability analysis tools in terms of accuracy, efficiency, and scalability, and (ii) our structure-aware estimators further outperform (blackbox) estimators that do not exploit the inherent program structure. We also identify multiple program properties that limit the applicability of the existing probabilistic analysis techniques.},
  isbn = {9798400703270},
  file = {/home/csoare/Zotero/storage/EYKE45TJ/Lee and Böhme - 2023 - Statistical Reachability Analysis.pdf}
}

@misc{meelSparseHashingScalable2020,
  title = {Sparse {{Hashing}} for {{Scalable Approximate Model Counting}}: {{Theory}} and {{Practice}}},
  shorttitle = {Sparse {{Hashing}} for {{Scalable Approximate Model Counting}}},
  author = {Meel, Kuldeep S. and Akshay, S.},
  year = {2020},
  month = apr,
  number = {arXiv:2004.14692},
  eprint = {2004.14692},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.14692},
  urldate = {2024-11-14},
  abstract = {Given a CNF formula F on n variables, the problem of model counting or \#SAT is to compute the number of satisfying assignments of F . Model counting is a fundamental but hard problem in computer science with varied applications. Recent years have witnessed a surge of effort towards developing efficient algorithmic techniques that combine the classical 2-universal hashing with the remarkable progress in SAT solving over the past decade. These techniques augment the CNF formula F with random XOR constraints and invoke an NP oracle repeatedly on the resultant CNF-XOR formulas. In practice, calls to the NP oracle calls are replaced a SAT solver whose runtime performance is adversely affected by size of XOR constraints. The standard construction of 2-universal hash functions chooses every variable with probability p = 1/2 leading to XOR constraints of size n/2 in expectation. Consequently, the challenge is to design sparse hash functions where variables can be chosen with smaller probability and lead to smaller sized XOR constraints. In this paper, we address this challenge from theoretical and practical perspectives. First, we formalize a relaxation of universal hashing, called concentrated hashing and establish a novel and beautiful connection between concentration measures of these hash functions and isoperimetric inequalities on boolean hypercubes. This allows us to obtain (log m) tight bounds on variance and dispersion index and show that p = O( log(m)/m ) suffices for design of sparse hash functions from \{0, 1\}{\textasciicircum}n to \{0, 1\}{\textasciicircum}m. We then use sparse hash functions belonging to this concentrated hash family to develop new approximate counting algorithms. A comprehensive experimental evaluation of our algorithm on 1893 benchmarks demonstrates that usage of sparse hash functions can lead to significant speedups.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  file = {/home/csoare/Zotero/storage/6MXLP6NI/Meel and Akshay - 2020 - Sparse Hashing for Scalable Approximate Model Counting Theory and Practice.pdf;/home/csoare/Zotero/storage/ZJZVQKXF/2004.html}
}

@misc{NewWidthParameters,
  title = {New Width Parameters for {{SAT}} and \#{{SAT}} - {{ScienceDirect}}},
  urldate = {2024-11-16},
  howpublished = {https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0004370221000114?via\%3Dihub},
  file = {/home/csoare/Zotero/storage/6Y29HQ76/S0004370221000114.html}
}

@inproceedings{nollerQFuzzQuantitativeFuzzing2021,
  title = {{{QFuzz}}: Quantitative Fuzzing for Side Channels},
  shorttitle = {{{QFuzz}}},
  booktitle = {Proceedings of the 30th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Noller, Yannic and {Tizpaz-Niari}, Saeid},
  year = {2021},
  month = jul,
  series = {{{ISSTA}} 2021},
  pages = {257--269},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3460319.3464817},
  urldate = {2024-11-15},
  abstract = {Side channels pose a significant threat to the confidentiality of software systems. Such vulnerabilities are challenging to detect and evaluate because they arise from non-functional properties of software such as execution times and require reasoning on multiple execution traces. Recently, noninterference notions have been adapted in static analysis, symbolic execution, and greybox fuzzing techniques. However, noninterference is a strict notion and may reject security even if the strength of information leaks are weak. A quantitative notion of security allows for the relaxation of noninterference and tolerates small (unavoidable) leaks. Despite progress in recent years, the existing quantitative approaches have scalability limitations in practice.  In this work, we present QFuzz, a greybox fuzzing technique to quantitatively evaluate the strength of side channels with a focus on min entropy. Min entropy is a measure based on the number of distinguishable observations (partitions) to assess the resulting threat from an attacker who tries to compromise secrets in one try. We develop a novel greybox fuzzing equipped with two partitioning algorithms that try to maximize the number of distinguishable observations and the cost differences between them.  We evaluate QFuzz on a large set of benchmarks from existing work and real-world libraries (with a total of 70 subjects). QFuzz compares favorably to three state-of-the-art detection techniques. QFuzz provides quantitative information about leaks beyond the capabilities of all three techniques. Crucially, we compare QFuzz to a state-of-the-art quantification tool and find that QFuzz significantly outperforms the tool in scalability while maintaining similar precision. Overall, we find that our approach scales well for real-world applications and provides useful information to evaluate resulting threats. Additionally, QFuzz identifies a zero-day side-channel vulnerability in a security critical Java library that has since been confirmed and fixed by the developers.},
  isbn = {978-1-4503-8459-9},
  file = {/home/csoare/Zotero/storage/6V2INI6H/Noller and Tizpaz-Niari - 2021 - QFuzz quantitative fuzzing for side channels.pdf}
}

@article{ordyniakSatisfiabilityAcyclicAlmost2013,
  title = {Satisfiability of Acyclic and Almost Acyclic {{CNF}} Formulas},
  author = {Ordyniak, Sebastian and Paulusma, Daniel and Szeider, Stefan},
  year = {2013},
  month = apr,
  journal = {Theoretical Computer Science},
  volume = {481},
  pages = {85--99},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2012.12.039},
  urldate = {2024-11-16},
  abstract = {We show that the Satisfiability (SAT) problem for CNF formulas with {$\beta$}-acyclic hypergraphs can be solved in polynomial time by using a special type of Davis--Putnam resolution in which each resolvent is a subset of a parent clause. We extend this class to CNF formulas for which this type of Davis--Putnam resolution still applies and show that testing membership in this class is NP-complete. We compare the class of {$\beta$}-acyclic formulas and this superclass with a number of known polynomial formula classes. We then study the parameterized complexity of SAT for ``almost'' {$\beta$}-acyclic instances, using as parameter the formula's distance from being {$\beta$}-acyclic. As distance we use the size of a smallest strong backdoor set and the {$\beta$}-hypertree width. As a by-product we obtain the W[1]-hardness of SAT parameterized by the (undirected) clique-width of the incidence graph, which disproves a conjecture by Fischer, Makowsky, and Ravve.},
  keywords = {Acyclic hypergraph,Chordal bipartite graph,Davis-Putnam resolution},
  file = {/home/csoare/Zotero/storage/N2EJY93N/Ordyniak et al. - 2013 - Satisfiability of acyclic and almost acyclic CNF formulas.pdf;/home/csoare/Zotero/storage/Y687KKRD/S0304397513000054.html}
}

@article{ordyniakSatisfiabilityAcyclicAlmost2013a,
  title = {Satisfiability of Acyclic and Almost Acyclic {{CNF}} Formulas},
  author = {Ordyniak, Sebastian and Paulusma, Daniel and Szeider, Stefan},
  year = {2013},
  month = apr,
  journal = {Theoretical Computer Science},
  volume = {481},
  pages = {85--99},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2012.12.039},
  urldate = {2024-11-16},
  abstract = {We show that the Satisfiability (SAT) problem for CNF formulas with {$\beta$}-acyclic hypergraphs can be solved in polynomial time by using a special type of Davis--Putnam resolution in which each resolvent is a subset of a parent clause. We extend this class to CNF formulas for which this type of Davis--Putnam resolution still applies and show that testing membership in this class is NP-complete. We compare the class of {$\beta$}-acyclic formulas and this superclass with a number of known polynomial formula classes. We then study the parameterized complexity of SAT for ``almost'' {$\beta$}-acyclic instances, using as parameter the formula's distance from being {$\beta$}-acyclic. As distance we use the size of a smallest strong backdoor set and the {$\beta$}-hypertree width. As a by-product we obtain the W[1]-hardness of SAT parameterized by the (undirected) clique-width of the incidence graph, which disproves a conjecture by Fischer, Makowsky, and Ravve.},
  keywords = {Acyclic hypergraph,Chordal bipartite graph,Davis-Putnam resolution},
  file = {/home/csoare/Zotero/storage/ULWWS8SR/Ordyniak et al. - 2013 - Satisfiability of acyclic and almost acyclic CNF formulas.pdf;/home/csoare/Zotero/storage/83LAL7BS/S0304397513000054.html}
}

@article{paulusmaModelCountingCNF2016,
  title = {Model {{Counting}} for {{CNF Formulas}} of {{Bounded Modular Treewidth}}},
  author = {Paulusma, Daniel and Slivovsky, Friedrich and Szeider, Stefan},
  year = {2016},
  month = sep,
  journal = {Algorithmica},
  volume = {76},
  number = {1},
  pages = {168--194},
  issn = {1432-0541},
  doi = {10.1007/s00453-015-0030-x},
  urldate = {2024-11-16},
  abstract = {We define the modular treewidth of a graph as its treewidth after contraction of modules. This parameter properly generalizes treewidth and is itself properly generalized by clique-width. We show that the number of satisfying assignments can be computed in polynomial time for CNF formulas whose incidence graphs have bounded modular treewidth. Our result generalizes known results for the treewidth of incidence graphs and is incomparable with known results for clique-width (or rank-width) of signed incidence graphs. The contraction of modules is an effective data reduction procedure. Our algorithm is the first one to harness this technique for \#SAT. The order of the polynomial bounding the runtime of our algorithm depends on the modular treewidth of the input formula. We show that it is unlikely that this dependency can be avoided by proving that SAT is W[1]-hard when parameterized by the modular incidence treewidth of the given CNF formula.},
  langid = {english},
  keywords = {Algorithms,Model Counting,Propositional Satisfiability}
}

@article{paulusmaModelCountingCNF2016a,
  title = {Model {{Counting}} for {{CNF Formulas}} of {{Bounded Modular Treewidth}}},
  author = {Paulusma, Daniel and Slivovsky, Friedrich and Szeider, Stefan},
  year = {2016},
  month = sep,
  journal = {Algorithmica},
  volume = {76},
  number = {1},
  pages = {168--194},
  issn = {1432-0541},
  doi = {10.1007/s00453-015-0030-x},
  urldate = {2024-11-16},
  abstract = {We define the modular treewidth of a graph as its treewidth after contraction of modules. This parameter properly generalizes treewidth and is itself properly generalized by clique-width. We show that the number of satisfying assignments can be computed in polynomial time for CNF formulas whose incidence graphs have bounded modular treewidth. Our result generalizes known results for the treewidth of incidence graphs and is incomparable with known results for clique-width (or rank-width) of signed incidence graphs. The contraction of modules is an effective data reduction procedure. Our algorithm is the first one to harness this technique for \#SAT. The order of the polynomial bounding the runtime of our algorithm depends on the modular treewidth of the input formula. We show that it is unlikely that this dependency can be avoided by proving that SAT is W[1]-hard when parameterized by the modular incidence treewidth of the given CNF formula.},
  langid = {english},
  keywords = {Algorithms,Model Counting,Propositional Satisfiability},
  file = {/home/csoare/Zotero/storage/D3DJMFGM/Paulusma et al. - 2016 - Model Counting for CNF Formulas of Bounded Modular Treewidth.pdf}
}

@article{porschenBaseHypergraphsOrbits2018,
  title = {Base {{Hypergraphs}} and {{Orbits}} of {{CNF Formulas}}},
  author = {Porschen, Stefan},
  year = {2018},
  journal = {Hong Kong},
  abstract = {The existence problem of specific base hypergraphs of formulas on basis of the fibre perspective on the propositional satisfiability problem is addressed. Further, the orbit structure of the set of fibre-transversals imposed by the flipping operation is investigated. Moreover some complexity results are proven. Methodogically, the concept of linear and exact linear hypergraphs, respectively, formulas is exploited.},
  langid = {english},
  file = {/home/csoare/Zotero/storage/65Q85FHQ/Porschen - 2018 - Base Hypergraphs and Orbits of CNF Formulas.pdf}
}

@article{porschenCNFStructureStabilizersOrbits2020,
  title = {{{CNF-Structure}}: {{Stabilizers}}, {{Orbits}}, {{Fibre-Transversals}}},
  author = {Porschen, Stefan},
  year = {2020},
  volume = {47},
  number = {2},
  abstract = {The structure of CNF formulas as well as classes of formulas is considered here from a group theoretic perspective, provided by the action under the complementation group. For this study again, the fibre view approach to CNF on basis of the concepts of base hypergraphs and its fibre-transversals is exploited extensively. Several CNF classes are investigated which are defined via orbits of complementation subgroups. Besides their stabilizer properties we also study to some extent the satisfiability aspects of those classes. In that context it turns out that several results regarding stabilizer properties or satisfiability aspects valid for fibre-transversals cannot be transfered when replacing them with arbitrary CNF formulas. Further we present an algorithm for computing the isotropy groups of fibre formulas, and investigate the lifting process to the total case. The members of several concrete subclasses of CNF are treated thereby such as linear or symmetric formulas.},
  langid = {english},
  file = {/home/csoare/Zotero/storage/ZPY2V9RI/Porschen - 2020 - CNF-Structure Stabilizers, Orbits, Fibre-Transversals.pdf}
}

@misc{saetherSolvingMaxSATSAT2014,
  title = {Solving {{MaxSAT}} and \#{{SAT}} on Structured {{CNF}} Formulas},
  author = {S{\ae}ther, Sigve Hortemo and Telle, Jan Arne and Vatshelle, Martin},
  year = {2014},
  month = feb,
  number = {arXiv:1402.6485},
  eprint = {1402.6485},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1402.6485},
  urldate = {2024-11-16},
  abstract = {In this paper we propose a structural parameter of CNF formulas and use it to identify instances of weighted MaxSAT and \#SAT that can be solved in polynomial time. Given a CNF formula we say that a set of clauses is precisely satisfiable if there is some complete assignment satisfying these clauses only. Let the ps-value of the formula be the number of precisely satisfiable sets of clauses. Applying the notion of branch decompositions to CNF formulas and using ps-value as cut function, we define the ps-width of a formula. For a formula given with a decomposition of polynomial ps-width we show dynamic programming algorithms solving weighted MaxSAT and \#SAT in polynomial time. Combining with results of 'Belmonte and Vatshelle, Graph classes with structured neighborhoods and algorithmic applications, Theor. Comput. Sci. 511: 54-65 (2013)' we get polynomial-time algorithms solving weighted MaxSAT and \#SAT for some classes of structured CNF formulas. For example, we get \$O(m{\textasciicircum}2(m + n)s)\$ algorithms for formulas \$F\$ of \$m\$ clauses and \$n\$ variables and size \$s\$, if \$F\$ has a linear ordering of the variables and clauses such that for any variable \$x\$ occurring in clause \$C\$, if \$x\$ appears before \$C\$ then any variable between them also occurs in \$C\$, and if \$C\$ appears before \$x\$ then \$x\$ occurs also in any clause between them. Note that the class of incidence graphs of such formulas do not have bounded clique-width.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms},
  file = {/home/csoare/Zotero/storage/D6JC9EWA/Sæther et al. - 2014 - Solving MaxSAT and #SAT on structured CNF formulas.pdf;/home/csoare/Zotero/storage/DVYBDRCD/1402.html}
}

@inproceedings{sahaRarePathGuided2023,
  title = {Rare {{Path Guided Fuzzing}}},
  booktitle = {{{ISSTA}} 2023 - {{Proceedings}} of the 32nd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Saha, S. and Sarker, L. and Shafiuzzaman, M. and Shou, C. and Li, A. and Sankaran, G. and Bultan, T.},
  year = {2023},
  pages = {1295--1306},
  doi = {10.1145/3597926.3598136},
  abstract = {Starting with a random initial seed, fuzzers search for inputs that trigger bugs or vulnerabilities. However, fuzzers often fail to generate inputs for program paths guarded by restrictive branch conditions. In this paper, we show that by first identifying rare-paths in programs (i.e., program paths with path constraints that are unlikely to be satisfied by random input generation), and then, generating inputs/seeds that trigger rare-paths, one can improve the coverage of fuzzing tools. In particular, we present techniques 1) that identify rare paths using quantitative symbolic analysis, and 2) generate inputs that can explore these rare paths using path-guided concolic execution. We provide these inputs as initial seed sets to three state of the art fuzzers. Our experimental evaluation on a set of programs shows that the fuzzers achieve better coverage with the rare-path based seed set compared to a random initial seed. {\copyright} 2023 Owner/Author.},
  keywords = {Concolic execution,Control flow analysis,Fuzz testing,Model counting,Probabilistic analysis},
  file = {/home/csoare/Zotero/storage/3HYMQHH4/display.html}
}

@inproceedings{sahaRarePathGuided2023a,
  title = {Rare {{Path Guided Fuzzing}}},
  booktitle = {Proceedings of the 32nd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Saha, Seemanta and Sarker, Laboni and Shafiuzzaman, Md and Shou, Chaofan and Li, Albert and Sankaran, Ganesh and Bultan, Tevfik},
  year = {2023},
  month = jul,
  series = {{{ISSTA}} 2023},
  pages = {1295--1306},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3597926.3598136},
  urldate = {2024-11-15},
  abstract = {Starting with a random initial seed, fuzzers search for inputs that trigger bugs or vulnerabilities. However, fuzzers often fail to generate inputs for program paths guarded by restrictive branch conditions. In this paper, we show that by first identifying rare-paths in programs (i.e., program paths with path constraints that are unlikely to be satisfied by random input generation), and then, generating inputs/seeds that trigger rare-paths, one can improve the coverage of fuzzing tools. In particular, we present techniques 1) that identify rare paths using quantitative symbolic analysis, and 2) generate inputs that can explore these rare paths using path-guided concolic execution. We provide these inputs as initial seed sets to three state of the art fuzzers. Our experimental evaluation on a set of programs shows that the fuzzers achieve better coverage with the rare-path based seed set compared to a random initial seed.},
  isbn = {9798400702211},
  file = {/home/csoare/Zotero/storage/PUIPXL5T/Saha et al. - 2023 - Rare Path Guided Fuzzing.pdf}
}

@misc{sahaRareSeedGenerationFuzzing2022,
  title = {Rare-{{Seed Generation}} for {{Fuzzing}}},
  author = {Saha, Seemanta and Sarker, Laboni and Shafiuzzaman, Md and Shou, Chaofan and Li, Albert and Sankaran, Ganesh and Bultan, Tevfik},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09004},
  eprint = {2212.09004},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.09004},
  urldate = {2024-11-15},
  abstract = {Starting with a random initial seed, fuzzers search for inputs that trigger bugs or vulnerabilities. However, fuzzers often fail to generate inputs for program paths guarded by restrictive branch conditions. In this paper, we show that by first identifying rare-paths in programs (i.e., program paths with path constraints that are unlikely to be satisfied by random input generation), and then, generating inputs/seeds that trigger rare-paths, one can improve the coverage of fuzzing tools. In particular, we present techniques 1) that identify rare paths using quantitative symbolic analysis, and 2) generate inputs that can explore these rare paths using path-guided concolic execution. We provide these inputs as initial seed sets to three state of the art fuzzers. Our experimental evaluation on a set of programs (that contain a lot of restrictive branch conditions) shows that the fuzzers achieve better coverage with the rare-path based seed set compared to a random initial seed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Software Engineering},
  file = {/home/csoare/Zotero/storage/6W6BPIJ4/Saha et al. - 2022 - Rare-Seed Generation for Fuzzing.pdf;/home/csoare/Zotero/storage/969MEP68/2212.html}
}

@article{shahriariTakingHumanOut2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {de Freitas}, Nando},
  year = {2016},
  month = jan,
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  pages = {148--175},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2015.2494218},
  urldate = {2024-11-17},
  abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  keywords = {Bayes methods,Big data,decision making,Decision making,design of experiments,Design of experiments,Genomes,genomic medicine,Linear programming,optimization,Optimization,response surface methodology,Statistical analysis,statistical learning},
  file = {/home/csoare/Zotero/storage/YZ79JVSB/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Bayesian Optimization.pdf;/home/csoare/Zotero/storage/WZP2W9Q9/7352306.html}
}

@inproceedings{shavitRevisitingSATZillaFeatures2024,
  title = {Revisiting {{SATZilla Features}} in 2024},
  booktitle = {{{SAT}}},
  author = {Shavit, Hadar and Hoos, Holger H.},
  year = {2024},
  month = jan,
  urldate = {2024-11-14},
  abstract = {Boolean satisfiability (SAT) is an NP-complete problem with important applications, notably in hardware and software verification. Characterising a SAT instance by a set of features has shown great potential for various tasks, ranging from algorithm selection to benchmark generation. In this work, we revisit the widely used SATZilla features and introduce a new version of the tool used to compute them. In particular, we utilise a new preprocessor and SAT solvers, adjust the code to accommodate larger formulas, and determine better settings of the feature extraction time limits. We evaluate the extracted features on three downstream tasks: satisfiability prediction, running time prediction, and algorithm selection. We observe that our new tool is able to extract features from a broader range of instances than before. We show that the new version of the feature extractor produces features that achieve up to 26\% lower RMSE for running time prediction, up to 3\% higher accuracy for satisfiability prediction, and up to 15 times higher closed gap for algorithm selection on benchmarks from recent SAT competitions.},
  langid = {english},
  file = {/home/csoare/Zotero/storage/W3LK2K24/Shavit and Hoos - 2024 - Revisiting SATZilla Features in 2024.pdf}
}

@misc{shawModelCountingWild2024,
  title = {Model {{Counting}} in the {{Wild}}},
  author = {Shaw, Arijit and Meel, Kuldeep S.},
  year = {2024},
  month = aug,
  number = {arXiv:2408.07059},
  eprint = {2408.07059},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.07059},
  urldate = {2024-11-13},
  abstract = {Model counting is a fundamental problem in automated reasoning with applications in probabilistic inference, network reliability, neural network verification, and more. Although model counting is computationally intractable from a theoretical perspective due to its \#P-completeness, the past decade has seen significant progress in developing state-of-the-art model counters to address scalability challenges. In this work, we conduct a rigorous assessment of the scalability of model counters in the wild. To this end, we surveyed 11 application domains and collected an aggregate of 2262 benchmarks from these domains. We then evaluated six state-of-the-art model counters on these instances to assess scalability and runtime performance. Our empirical evaluation demonstrates that the performance of model counters varies significantly across different application domains, underscoring the need for careful selection by the end user. Additionally, we investigated the behavior of different counters with respect to two parameters suggested by the model counting community, finding only a weak correlation. Our analysis highlights the challenges and opportunities for portfolio-based approaches in model counting.},
  archiveprefix = {arXiv},
  keywords = {Applications of SAT,Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Model Counting,SAT},
  file = {/home/csoare/Zotero/storage/L3DZ9MJ3/Shaw and Meel - 2024 - Model Counting in the Wild.pdf;/home/csoare/Zotero/storage/6PDJ9JS5/2408.html}
}

@misc{UnitPropagationOverview,
  title = {Unit {{Propagation}} - an Overview {\textbar} {{ScienceDirect Topics}}},
  urldate = {2024-11-16},
  howpublished = {https://www-sciencedirect-com.tudelft.idm.oclc.org/topics/computer-science/unit-propagation},
  file = {/home/csoare/Zotero/storage/7MVCHLMW/unit-propagation.html}
}

@inproceedings{usmanTestMCTestingModel2020,
  title = {{{TestMC}}: {{Testing Model Counters}} Using {{Differential}} and {{Metamorphic Testing}}},
  shorttitle = {{{TestMC}}},
  booktitle = {2020 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Usman, Muhammad and Wang, Wenxi and Khurshid, Sarfraz},
  year = {2020},
  month = sep,
  pages = {709--721},
  issn = {2643-1572},
  urldate = {2024-11-14},
  abstract = {Model counting is the problem for finding the number of solutions to a formula over a bounded universe. This is a classic problem in computer science that has seen many recent advances in techniques and tools that tackle it. These advances have led to applications of model counting in many domains, e.g., quantitative program analysis, reliability, and security. Given the sheer complexity of the underlying problem, today's model counters employ sophisticated algorithms and heuristics, which result in complex tools that must be heavily optimized. Therefore, establishing the correctness of implementations of model counters necessitates rigorous testing. This experience paper presents an empirical study on testing industrial strength model counters by applying the principles of differential and metamorphic testing together with bounded exhaustive input generation and input minimization. We embody these principles in the TestMC framework, and apply it to test four model counters, including three state-of-the-art model counters from three different classes. Specifically, we test the exact model counters projMC and dSharp, the probabilistic exact model counter Ganak, and the probabilistic approximate model counter ApproxMC. As subjects, we use three complementary test suites of input formulas. One suite consists of larger formulas that are derived from a wide range of real-world software design problems. The second suite consists of a bounded exhaustive set of small formulas that TestMC generated. The third suite consists of formulas generated using an off-the-shelf CNF fuzzer. TestMC found bugs in three of the four subject model counters. The bugs led to crashes, segmentation faults, incorrect model counts, and resource exhaustion by the solvers. Two of the tools were corrected subsequent to the bug reports we submitted based on our study, whereas the bugs we reported in the third tool were deemed by the tool authors to not require a fix.},
  keywords = {Computational modeling,Computer bugs,delta debugging,delta-debugging,differential testing,metamorphic testing,Minimization,Model counting,Probabilistic logic,Software design,Testing,testing model counters,TestMC,Tools},
  file = {/home/csoare/Zotero/storage/R8HJNL43/Usman et al. - 2020 - TestMC Testing Model Counters using Differential and Metamorphic Testing.pdf;/home/csoare/Zotero/storage/BKN55DV3/9286130.html}
}

@article{valiantComplexityComputingPermanent1979,
  title = {The Complexity of Computing the Permanent},
  author = {Valiant, L. G.},
  year = {1979},
  month = jan,
  journal = {Theoretical Computer Science},
  volume = {8},
  number = {2},
  pages = {189--201},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(79)90044-6},
  urldate = {2024-11-13},
  abstract = {It is shown that the permanent function of (0, 1)-matrices is a complete problem for the class of counting problems associated with nondeterministic polynomial time computations. Related counting problems are also considered. The reductions used are characterized by their nontrivial use of arithmetic.},
  keywords = {Complexity,SAT,Theory},
  file = {/home/csoare/Zotero/storage/5PDSNEE8/Valiant - 1979 - The complexity of computing the permanent.pdf;/home/csoare/Zotero/storage/LQ7D77N6/0304397579900446.html}
}

@article{xuSATzillaPortfoliobasedAlgorithm2008,
  title = {{{SATzilla}}: Portfolio-Based Algorithm Selection for {{SAT}}},
  shorttitle = {{{SATzilla}}},
  author = {Xu, Lin and Hutter, Frank and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  year = {2008},
  month = jun,
  journal = {J. Artif. Int. Res.},
  volume = {32},
  number = {1},
  pages = {565--606},
  issn = {1076-9757},
  abstract = {It has been widely observed that there is no single "dominant" SAT solver; instead, different solvers perform best on different instances. Rather than following the traditional approach of choosing the best solver for a given class of instances, we advocate making this decision online on a per-instance basis. Building on previous work, we describe SATzilla, an automated approach for constructing per-instance algorithm portfolios for SAT that use so-called empirical hardness models to choose among their constituent solvers. This approach takes as input a distribution of problem instances and a set of component solvers, and constructs a portfolio optimizing a given objective function (such as mean runtime, percent of instances solved, or score in a competition). The excellent performance of SATzilla was independently verified in the 2007 SAT Competition, where our SATzilla07 solvers won three gold, one silver and one bronze medal. In this article, we go well beyond SATzilla07 by making the portfolio construction scalable and completely automated, and improving it by integrating local search solvers as candidate solvers, by predicting performance score instead of runtime, and by using hierarchical hardness models that take into account different types of SAT instances. We demonstrate the effectiveness of these new techniques in extensive experimental results on data sets including instances from the most recent SAT competition.},
  file = {/home/csoare/Zotero/storage/22H3QGT8/Xu et al. - 2008 - SATzilla portfolio-based algorithm selection for SAT.pdf}
}

@article{zellerYesterdayMyProgram1999,
  title = {Yesterday, My Program Worked. {{Today}}, It Does Not. {{Why}}?},
  author = {Zeller, Andreas},
  year = {1999},
  month = oct,
  journal = {SIGSOFT Softw. Eng. Notes},
  volume = {24},
  number = {6},
  pages = {253--267},
  issn = {0163-5948},
  doi = {10.1145/318774.318946},
  urldate = {2024-11-17},
  abstract = {Imagine some program and a number of changes. If none of these changes is applied (``yesterday''), the program works. If all changes are applied (``today''), the program does not work. Which change is responsible for the failure? We present an efficient algorithm that determines the minimal set of failure-inducing changes. Our delta debugging prototype tracked down a single failure-inducing change from 178,000 changed GDB lines within a few hours.},
  file = {/home/csoare/Zotero/storage/GVD6497N/Zeller - 1999 - Yesterday, my program worked. Today, it does not. Why.pdf}
}

@article{zhangStructuralEntropyMeasurement2021,
  title = {A {{Structural Entropy Measurement Principle}} of {{Propositional Formulas}} in {{Conjunctive Normal Form}}},
  author = {Zhang, Zaijun and Xu, Daoyun and Zhou, Jincheng},
  year = {2021},
  month = mar,
  journal = {Entropy},
  volume = {23},
  number = {3},
  pages = {303},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e23030303},
  urldate = {2024-11-16},
  abstract = {The satisfiability (SAT) problem is a core problem in computer science. Existing studies have shown that most industrial SAT instances can be effectively solved by modern SAT solvers while random SAT instances cannot. It is believed that the structural characteristics of different SAT formula classes are the reasons behind this difference. In this paper, we study the structural properties of propositional formulas in conjunctive normal form (CNF) by the principle of structural entropy of formulas. First, we used structural entropy to measure the complex structure of a formula and found that the difficulty solving the formula is related to the structural entropy of the formula. The smaller the compressing information of a formula, the more difficult it is to solve the formula. Secondly, we proposed a {$\lambda$}-approximation strategy to approximate the structural entropy of large formulas. The experimental results showed that the proposed strategy can effectively approximate the structural entropy of the original formula and that the approximation ratio is more than 92\%. Finally, we analyzed the structural properties of a formula in the solution process and found that a local search solver tends to select variables in different communities to perform the next round of searches during a search and that the structural entropy of a variable affects the probability of the variable being flipped. By using these conclusions, we also proposed an initial candidate solution generation strategy for a local search for SAT, and the experimental results showed that this strategy effectively improves the performance of the solvers CCAsat and Sparrow2011 when incorporated into these two solvers.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {CNF formula,SAT problem,structural entropy,structural properties,the structural complexity},
  file = {/home/csoare/Zotero/storage/HNUYK7GI/Zhang et al. - 2021 - A Structural Entropy Measurement Principle of Propositional Formulas in Conjunctive Normal Form.pdf}
}

@misc{zotero-81,
  type = {Misc}
}
